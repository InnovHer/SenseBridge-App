# SenseBridge-App
SenseBridge

SenseBridge is an accessibility-first AI application designed to support blind, deaf, and deaf-blind users by breaking communication barriers through vision, speech, language, and haptic technologies.

The app focuses on clarity, safety, and inclusivity, providing multi-sensory interaction so users do not need to rely on sight or hearing alone.

ğŸš€ Project Overview

Many accessibility tools only address a single sensory need. SenseBridge combines multiple AI technologies into one cohesive system that supports:

Visual accessibility

Audio accessibility

Touch-based (haptic) communication

SenseBridge was built as an End-to-End AI Solution Capstone Project.

ğŸ¯ Key Features
Accessibility & UI

High-contrast interface optimized for low vision

Extra-large touch targets

Simple and predictable navigation

Braille text representation

Emergency one-tap access

Daily routine and reminder tracker

Communication Tools

Communication cards for quick messages

Multi-sensory feedback (visual, audio, haptic)

Haptic language system using vibration patterns

ğŸ¤– AI Technologies Used

SenseBridge integrates at least three AI technologies, meeting the project requirements:

Speech-to-Text
Converts spoken language into readable text for deaf users.

Text-to-Speech
Reads text aloud for blind users using natural-sounding voices.

Vision AI (Image Description)
Analyzes images and provides clear scene descriptions for blind users.

Language AI (Optional Enhancement)
Simplifies and summarizes text to improve readability.

ğŸ§  Haptic Language System

SenseBridge introduces a simple haptic language system that communicates meaning through vibration patterns:

Short vibration: confirmation

Double vibration: new message

Long vibration: reminder

Repeating vibration: emergency alert

This allows deaf-blind users to receive information without relying on sound or visuals.

ğŸ—ï¸ System Architecture

Frontend (Mobile App):

Accessible UI

Camera, microphone, and touch input

Text, audio, Braille, and haptic output

Backend:

API coordination and business logic

Error handling and fallbacks

Monitoring and logging

AI Services:

Speech-to-Text API

Text-to-Speech API

Vision AI API

Language Model API

Database:

Stores user preferences

Schedules and reminders

Communication history

ğŸ”„ Application Flow

User interacts with the app (voice, text, image, or touch)

Frontend sends data to backend

Backend routes requests to appropriate AI services

AI processes input and returns results

Backend validates, logs, and stores data

Results are delivered via text, audio, Braille, or haptics

ğŸ” Security & Privacy

Minimal data collection

Encrypted storage for user data

No permanent storage of raw images or audio

Ethical AI usage principles applied

ğŸ§ª Testing Strategy

Manual testing of all accessibility features

Scenario-based testing (blind, deaf, deaf-blind users)

API failure and fallback testing

Emergency feature testing

ğŸ“ˆ Scalability & Future Improvements

Planned enhancements include:

Wearable device integration

Offline emergency mode

Advanced AI personalization

Community-shared communication cards

Additional language and Braille support

ğŸ› ï¸ Setup & Installation (Prototype)

Clone the repository

Install dependencies

Configure API keys in environment variables

Run the backend server

Launch the frontend application

(Full setup instructions are provided in the project documentation.)

ğŸ¥ Demo

A 7-minute demonstration video is included with the project submission, showcasing:

Core features

AI integrations

Accessibility design choices

Real-world user scenarios

ğŸ‘¥ Team & Credits

This project was developed as part of the Tech Career Accelerator Capstone Project.

ğŸ“„ License

This project is for educational purposes only.
